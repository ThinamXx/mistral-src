{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MASK GENERATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xformers.ops.fmha.attn_bias import (\n",
    "    BlockDiagonalCausalMask,\n",
    "    BlockDiagonalCausalWithOffsetPaddedKeysMask,\n",
    "    BlockDiagonalMask,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., -inf, -inf, -inf],\n",
       "         [0., 0., -inf, -inf],\n",
       "         [-inf, -inf, 0., -inf],\n",
       "         [-inf, -inf, 0., 0.]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# debugging BlockDiagonalCausalMask\n",
    "seqlens = [2, 2]\n",
    "sliding_window = 2\n",
    "batch_size = 1\n",
    "total_seq = sum(seqlens)\n",
    "\n",
    "mask = BlockDiagonalCausalMask.from_seqlens(seqlens).make_local_attention(sliding_window)\n",
    "mask.materialize((batch_size, total_seq, total_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-inf, 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "         [-inf, -inf, 0., 0., -inf, -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, -inf, -inf, 0., 0., -inf],\n",
       "         [-inf, -inf, -inf, -inf, -inf, -inf, 0., 0.]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# debugging BlockDiagonalMask\n",
    "seqlens = [2, 2]\n",
    "kv_seqlens = [4, 4]\n",
    "sliding_window = 2\n",
    "batch_size = 1\n",
    "total_seq = sum(seqlens)\n",
    "\n",
    "mask = BlockDiagonalMask.from_seqlens(\n",
    "                q_seqlen=seqlens,\n",
    "                kv_seqlen=kv_seqlens,\n",
    "            ).make_local_attention_from_bottomright(sliding_window)\n",
    "mask.materialize((batch_size, total_seq, sum(kv_seqlens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# debugging\n",
    "seqlens = [2, 2]\n",
    "kv_seqlens = [4, 4]\n",
    "kv_padding = 9\n",
    "sliding_window = 2\n",
    "batch_size = 1\n",
    "total_seq = sum(seqlens)\n",
    "\n",
    "mask = BlockDiagonalCausalWithOffsetPaddedKeysMask.from_seqlens(\n",
    "                q_seqlen=seqlens,\n",
    "                kv_padding=kv_padding,\n",
    "                kv_seqlen=kv_seqlens\n",
    "            )\n",
    "mask.materialize((batch_size, total_seq, kv_padding * len(kv_seqlens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SLIDING WINDOW ATTENTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'I'}, {'am'}, {'living'}, {'in'}, {'Nepal'}]\n"
     ]
    }
   ],
   "source": [
    "# create the sequence\n",
    "example = [\"I\", \"am\", \"living\", \"in\", \"Nepal\"]\n",
    "sequence = [{example[i]} for i in range(len(example))]\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'I'}, None, None, None, None],\n",
       " [{'I', 'am'}, {'am'}, None, None, None],\n",
       " [{'I', 'living'}, {'am', 'living'}, {'living'}, None, None],\n",
       " [None, {'am', 'in'}, {'in', 'living'}, {'in'}, None],\n",
       " [None, None, {'Nepal', 'living'}, {'Nepal', 'in'}, {'Nepal'}]]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the sliding window attention\n",
    "sliding_window = 3\n",
    "\n",
    "def sliding_window_attention(seq: list[set[str]], w: int):\n",
    "    seq_len = len(seq)\n",
    "    attn_scores: list[list[set]] = [[None for _ in range(seq_len)] for _ in range(seq_len)]\n",
    "    \n",
    "    for i, q_tokens in enumerate(seq):\n",
    "        for j, k_tokens in enumerate(seq):\n",
    "            # if j > i, then we are looking at the future tokens\n",
    "            if j > i:\n",
    "                continue\n",
    "            # if i - j > w, then we are looking at tokens that are too far away\n",
    "            if i - j >= w:\n",
    "                continue\n",
    "            \n",
    "            attention = set()\n",
    "            attention.update(q_tokens)\n",
    "            attention.update(k_tokens)\n",
    "            attn_scores[i][j] = attention\n",
    "            \n",
    "    return attn_scores\n",
    "\n",
    "# create the attention scores\n",
    "attn_scores = sliding_window_attention(sequence, sliding_window)\n",
    "attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'I'},\n",
       " {'I', 'am'},\n",
       " {'I', 'am', 'living'},\n",
       " {'am', 'in', 'living'},\n",
       " {'Nepal', 'in', 'living'}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiply the attention scores with the values\n",
    "def multiply_attn_scores_with_values(attn_scores: list[list[set]], v_seq: list[set[str]]) -> list[set[str]]:\n",
    "    seq_len = len(v_seq)\n",
    "    result = [set() for _ in range(seq_len)]\n",
    "    \n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            attention = attn_scores[i][j]\n",
    "            v = v_seq[j]\n",
    "            r = result[i]\n",
    "            \n",
    "            if attention is not None:\n",
    "                r.update(v)\n",
    "                r.update(attention)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# create the attention\n",
    "v_seq = sequence\n",
    "result = multiply_attn_scores_with_values(attn_scores, v_seq)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I']\tNone\tNone\tNone\tNone\t\n",
      "['I', 'am']\t['am']\tNone\tNone\tNone\t\n",
      "['I', 'living']\t['am', 'living']\t['living']\tNone\tNone\t\n",
      "None\t['am', 'in']\t['living', 'in']\t['in']\tNone\t\n",
      "None\tNone\t['living', 'Nepal']\t['in', 'Nepal']\t['Nepal']\t\n"
     ]
    }
   ],
   "source": [
    "# inspect the attention\n",
    "def print_attention(attn_scores: list[list[set[str]]]):\n",
    "    for i, row in enumerate(attn_scores):\n",
    "        for j, attention in enumerate(row):\n",
    "            if attention is None:\n",
    "                print(\"None\", end=\"\\t\")\n",
    "            else:\n",
    "                print(f\"{sorted(attention, key=lambda x: example.index(x))}\", end=\"\\t\")\n",
    "        print()\n",
    "\n",
    "# print the attention\n",
    "print_attention(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ['I']\n",
      "1: ['am']\n",
      "2: ['living']\n",
      "3: ['in']\n",
      "4: ['Nepal']\n"
     ]
    }
   ],
   "source": [
    "# print the sequence:\n",
    "def print_sequence(seq: list[set[str]]):\n",
    "    for i, tokens in enumerate(seq):\n",
    "        print(f\"{i}: {sorted(tokens, key=lambda x: example.index(x))}\")\n",
    "\n",
    "# print the sequence\n",
    "print_sequence(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 input:\n",
      "0: ['I']\n",
      "1: ['am']\n",
      "2: ['living']\n",
      "3: ['in']\n",
      "4: ['Nepal']\n",
      "\n",
      "Layer 1 attention scores:\n",
      "['I']\tNone\tNone\tNone\tNone\t\n",
      "['I', 'am']\t['am']\tNone\tNone\tNone\t\n",
      "['I', 'living']\t['am', 'living']\t['living']\tNone\tNone\t\n",
      "None\t['am', 'in']\t['living', 'in']\t['in']\tNone\t\n",
      "None\tNone\t['living', 'Nepal']\t['in', 'Nepal']\t['Nepal']\t\n",
      "\n",
      "Layer 1 output\n",
      "0: ['I']\n",
      "1: ['I', 'am']\n",
      "2: ['I', 'am', 'living']\n",
      "3: ['am', 'living', 'in']\n",
      "4: ['living', 'in', 'Nepal']\n"
     ]
    }
   ],
   "source": [
    "# print the layer output:\n",
    "def print_layer_output(input: list[set[str]], layer: int):\n",
    "    print(f\"Layer {layer} input:\")\n",
    "    print_sequence(input)\n",
    "    print()\n",
    "    attn_scores = sliding_window_attention(input, sliding_window)\n",
    "    print(f\"Layer {layer} attention scores:\")\n",
    "    print_attention(attn_scores)\n",
    "    print()\n",
    "    output = multiply_attn_scores_with_values(attn_scores, input)\n",
    "    print(f\"Layer {layer} output\")\n",
    "    print_sequence(output)\n",
    "    return output\n",
    "\n",
    "# print the layer output\n",
    "layer1_output = print_layer_output(sequence, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
